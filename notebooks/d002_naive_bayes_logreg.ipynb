{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes combined with logistic regression based on Jeremy Howard's notebook [here](https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline-eda-0-052-lb)\n",
    "\n",
    "Also refining naive bayes hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from __future__ import division, print_function \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif, SelectKBest\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import binarize \n",
    "from sklearn.metrics import log_loss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from evaluation import multilabel_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define classes and load test/train data, some input text is \"N/A\" so turn na_filter off to prevent this being converted to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_classes = [\n",
    "    'toxic', 'severe_toxic', 'obscene', \n",
    "    'threat', 'insult', 'identity_hate' \n",
    "]\n",
    "\n",
    "df = pd.read_csv('../data/train.csv', na_filter=False)\n",
    "X_train_text = df['comment_text'].values\n",
    "Y_train = df[toxic_classes].values\n",
    "id_train = df['id']\n",
    "\n",
    "df = pd.read_csv('../data/test.csv', na_filter=False)\n",
    "X_test_text = df['comment_text'].values\n",
    "id_test = df['id']\n",
    "\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will initially try to improve m000 by changing the standard hyperparameters / pre-processing for our simple MultinomialNB model, then later implement the combined shown in the Jeremy Howard notebook.\n",
    "Overview of differences between jhoward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(ngram_range=(1,2),\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1)\n",
    "\n",
    "X_train = tvec.fit_transform(X_train_text)\n",
    "X_test = tvec.transform(X_test_text)\n",
    "\n",
    "del(X_train_text)\n",
    "del(X_test_text)\n",
    "\n",
    "print('number of features:', len(tvec.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation for competition is mean of the log loss across all classes. Computing metric across 10 folds to give us our baseline score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.18786538228255142, 0.0080973807398286302)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores = cross_validate_multilabel(MultinomialNB(), X_train, Y_train, cv=10, scoring='neg_log_loss')\n",
    "np.mean(cv_scores), np.std(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting models on all data, then applying to test data to give us our probabilities for submission\n",
    "\n",
    "We use OneVsRestClassifier to fit one model per class as MultinomialNB cannot handle a multi-label input by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "ovr_nb = OneVsRestClassifier(nb_model)\n",
    "ovr_nb.fit(X_train, Y_train) \n",
    "ovr_nb.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.52842777e-05,   1.52804881e-10,   1.26619762e-05,\n",
       "          2.38209407e-10,   3.36363407e-06,   3.01831396e-08],\n",
       "       [  6.66552593e-16,   3.46862042e-30,   3.85420157e-18,\n",
       "          5.95280927e-22,   6.38911831e-20,   9.44841693e-21],\n",
       "       [  5.13770247e-19,   1.04641141e-42,   9.34651433e-23,\n",
       "          1.00644397e-41,   7.25965785e-23,   6.35388326e-38],\n",
       "       ..., \n",
       "       [  3.06263881e-06,   5.02402659e-11,   9.02998535e-07,\n",
       "          2.34925660e-11,   5.77075874e-07,   1.24178475e-09],\n",
       "       [  1.64277107e-02,   1.97501507e-02,   2.54775188e-02,\n",
       "          2.17313876e-02,   2.30510110e-02,   3.94433234e-02],\n",
       "       [  1.78417474e-21,   8.62141034e-55,   2.28563059e-26,\n",
       "          4.45122550e-60,   3.05508745e-29,   4.89934988e-51]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_prob = ovr_nb.predict_proba(X_test)\n",
    "Y_test_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that predict_proba function returns the normalised probabilities over each one-vs-rest class distribution.\n",
    "\n",
    "In other words, we don't use the raw \"posterior\" probability calculated by the MultinomialNB `P(x|y)•P(y)`  as these values will be very small, and is not a realistic probability anyway as this does not include the \"evidence\" term P(x) in the Bayes equation as this is fixed for all classes. Instead this value is normalised so that the \"probabilities\" for each record sum to ~1 within each binary class decision. So the probability `P(yi|Xi) + P(not(yi)|Xi) ≈ 1`.\n",
    "\n",
    "Note that as this is a multilabel classification for the one-vs-rest estimator the probability  is normalised within each class, and not across _all_ classes, so the sum of all probabilities across all classes does not have to sum to 1. For example we can have `P(y1|xi) = P(y2|xi) ≈ 1`  etc which gives us a high probability of that record belonging to multiple classes. Equally it is valid for the probability of a record to have low probabilities across all classes, suggesting it does not belong to any of the classes (we have many training examples that exhibit this characteristic). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>2.528428e-05</td>\n",
       "      <td>1.528049e-10</td>\n",
       "      <td>1.266198e-05</td>\n",
       "      <td>2.382094e-10</td>\n",
       "      <td>3.363634e-06</td>\n",
       "      <td>3.018314e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>6.665526e-16</td>\n",
       "      <td>3.468620e-30</td>\n",
       "      <td>3.854202e-18</td>\n",
       "      <td>5.952809e-22</td>\n",
       "      <td>6.389118e-20</td>\n",
       "      <td>9.448417e-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>5.137702e-19</td>\n",
       "      <td>1.046411e-42</td>\n",
       "      <td>9.346514e-23</td>\n",
       "      <td>1.006444e-41</td>\n",
       "      <td>7.259658e-23</td>\n",
       "      <td>6.353883e-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>6.786617e-03</td>\n",
       "      <td>1.692325e-05</td>\n",
       "      <td>1.369010e-03</td>\n",
       "      <td>6.565989e-05</td>\n",
       "      <td>7.297767e-04</td>\n",
       "      <td>5.367645e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>2.723515e-02</td>\n",
       "      <td>4.956793e-03</td>\n",
       "      <td>2.108912e-02</td>\n",
       "      <td>4.873167e-03</td>\n",
       "      <td>1.588704e-02</td>\n",
       "      <td>1.861962e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id         toxic  severe_toxic       obscene        threat  \\\n",
       "0   6044863  2.528428e-05  1.528049e-10  1.266198e-05  2.382094e-10   \n",
       "1   6102620  6.665526e-16  3.468620e-30  3.854202e-18  5.952809e-22   \n",
       "2  14563293  5.137702e-19  1.046411e-42  9.346514e-23  1.006444e-41   \n",
       "3  21086297  6.786617e-03  1.692325e-05  1.369010e-03  6.565989e-05   \n",
       "4  22982444  2.723515e-02  4.956793e-03  2.108912e-02  4.873167e-03   \n",
       "\n",
       "         insult  identity_hate  \n",
       "0  3.363634e-06   3.018314e-08  \n",
       "1  6.389118e-20   9.448417e-21  \n",
       "2  7.259658e-23   6.353883e-38  \n",
       "3  7.297767e-04   5.367645e-06  \n",
       "4  1.588704e-02   1.861962e-03  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submit = pd.concat([id_test, pd.DataFrame(Y_test_prob, columns=toxic_classes)], axis=1)\n",
    "df_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submit.to_csv('../results/m000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
